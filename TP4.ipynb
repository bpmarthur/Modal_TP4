{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf477e9-842c-41c8-901b-728340e0b509",
   "metadata": {},
   "source": [
    "# Lab 4: Information Extraction\n",
    "\n",
    "\n",
    "**Exercise 1.** Implement a trie that allows the operations *insert* and *find*. Add all the entities from the file *winners.txt* to the trie. Using this trie, perform entity recognition in the text file *example-nobel.txt*. The files are on Moodle.\n",
    "\n",
    "You can use as starting point the following code: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106d618",
   "metadata": {},
   "source": [
    "Lien de la ressource pour les Regex : https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4bc536a-1bbc-4994-9a14-64a9931b9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "\t\"\"\"A node in the trie structure\"\"\"\n",
    "\n",
    "\tdef __init__(self, char):\n",
    "\t\t# the character stored in this node\n",
    "\t\tself.char = char\n",
    "\n",
    "\t\t# whether this can be the end of a word\n",
    "\t\tself.is_end = False\n",
    "\n",
    "\t\t# a dictionary of child nodes\n",
    "\t\t# keys are characters, values are nodes\n",
    "\t\tself.children = {}\n",
    "\n",
    "\n",
    "class Trie(object):\n",
    "\t\"\"\"The trie object\"\"\"\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tThe trie has at least the root node.\n",
    "\t\tThe root node does not store any character\n",
    "\t\t\"\"\"\n",
    "\t\tself.root = TrieNode(\"\")\n",
    "\n",
    "\tdef insert(self, word: str):\n",
    "\t\t\"\"\"Insert a word into the trie\"\"\"\n",
    "\n",
    "\t\t# TO COMPLETE\n",
    "\t\tnode = self.root\n",
    "\n",
    "\t\t#On avance au fur et a mesure\n",
    "\t\tfor i in range(len(word)):\n",
    "\t\t\t\n",
    "\t\t\tif word[i] in node.children:\n",
    "\t\t\t\tnode = node.children[word[i]]\n",
    "\t\t\telse:\n",
    "\t\t\t\tchild = TrieNode(word[i])\n",
    "\t\t\t\tnode.children[word[i]] = child\n",
    "\t\t\t\tnode = child\n",
    "\n",
    "\t\tnode.is_end = True\t#Le dernier noeud termine forcement\n",
    "\n",
    "\t\t\t\n",
    "\tdef find(self, x):\n",
    "\t\t\"\"\"Return a tuple of two boolean values:\n",
    "\t\t  - first value: true or false if the prefix is present or not in the trie.\n",
    "\t\t  - second value: true if the prefix is a word in the trie\n",
    "\t\t  Example:\n",
    "\t\t\t  Suppose Adams is in our trie t.\n",
    "\t\t\t  t.find(\"Adams\") returns (True, True)\n",
    "\t\t\t  t.find(\"Adam\") returns (True, False)\n",
    "\t\t  \"\"\"\n",
    "\t\t\n",
    "\t\t# TO COMPLETE\n",
    "\t\tword = x\n",
    "\t\tval_1, val_2 = False, False\n",
    "\t\tnode = self.root\n",
    "\t\tfor i in range(len(word)):\n",
    "\t\t\tif word[i] in node.children:\n",
    "\t\t\t\tnode = node.children[word[i]]\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn False, False\n",
    "\t\t\n",
    "\t\treturn True, node.is_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa500e-ca57-46ae-8005-d764b583506d",
   "metadata": {},
   "source": [
    "**Exercise 2.** Supposing that all entities are written such that they start with a capital letter, perform entity recognition in the text file *example-nobel.txt* using regular expression. You should also be able to capture with the regex that an entity can have 2 or more names, such as Barack Obama. For this, first follow this tutorial https://www.w3schools.com/python/python_regex.asp on the **re library** in Python. When you have an idea for a regex, you can test it using this website https://regex101.com/ . Note the website even generates the corresponding Python code for you! You might need to read a bit about repeating a group https://www.regular-expressions.info/captureall.html ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b51cd66-a328-4457-aef0-ca6e546da82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Henry Kissinger', 'Le', 'Duc', 'of', 'Kissinger', 'North', 'North', 'Kissinger', 'North', 'of', 'Yasser', 'Shimon', 'Yitzhak Rabin', 'of', 'Arafat', 'Arafat', 'Barack Obama', 'Obama', 'of', 'Obama', 'of', 'of', 'Obama', 'Jimmy Carter', 'Al', 'of']\n",
      "['Nobel Peace', 'Henry Kissinger', 'Le Duc', 'Norwegian Nobel', 'North Vietnam', 'United States', 'Yasser Arafat', 'Shimon Peres', 'Yitzhak Rabin', 'Peace Prize', 'Norwegian Nobel', 'Peace Prize', 'Barack Obama', 'United States', 'Past Peace', 'Peace Prizes', 'Jimmy Carter', 'Al Gore']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Solution exercise 2\n",
    "def build_trie_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file and returns a trie containing all the words in the file.\n",
    "    Each word is assumed to be on a separate line.\n",
    "    \"\"\"\n",
    "    trie = Trie()\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            clean_line = line.strip()\n",
    "            ''' '''\n",
    "            #Utile si on ne veut qu'une petite partie du code\n",
    "            words = clean_line.split() \n",
    "            for word in words:\n",
    "                trie.insert(word)\n",
    "            \n",
    "\n",
    "            #Si on veut garder les mots composes\n",
    "            trie.insert(clean_line)\n",
    "    return trie\n",
    "\n",
    "def track_with_trie(file_train, file_tracked = \"./data/example-nobel.txt\"):\n",
    "    list = []\n",
    "    trie = build_trie_from_file(file_train)\n",
    "    #On cherche maintenant les noms propres du fichier example-nobel.txt\n",
    "    file = open(file_tracked, 'r')\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            word = words[i]\n",
    "            #print(f'{i} : {word}') #Used for debug\n",
    "            a, b = trie.find(word)\n",
    "            longest_word = None\n",
    "            next_i = i+1\n",
    "            #Si notre mot est un npm propre\n",
    "            if b:\n",
    "                longest_word = word\n",
    "            if a:\n",
    "                j = i+1\n",
    "                while a:\n",
    "                    a, b = trie.find(\" \".join(words[i:j+1]))\n",
    "                    #print(f'long : {longest_word}, thing : {\" \".join(words[i:j+1])}, a : {a}, b: {b}')  #Used for debug\n",
    "                    if b:\n",
    "                        longest_word = \" \".join(words[i:j+1])\n",
    "                        next_i = j+1\n",
    "                    j+=1\n",
    "            if longest_word != None:\n",
    "                list.append(longest_word)\n",
    "\n",
    "            i = next_i\n",
    "    return list\n",
    "\n",
    "\n",
    "print(track_with_trie(\"./data/winners.txt\"))\n",
    "\n",
    "def track_with_re(file_tracked):\n",
    "    file = open(file_tracked, 'r')\n",
    "    list = []\n",
    "    for line in file:\n",
    "        x = re.findall(\"[A-Z][a-z]+ [A-Z][a-z]+\", line.strip())\n",
    "        #x = re.findall(\"[A-Z][a-z]+ \", line.strip())\n",
    "        list += x\n",
    "    return list\n",
    "\n",
    "print(track_with_re(\"./data/example-nobel.txt\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547141be-2d83-440a-9c93-e42d6d573a3a",
   "metadata": {},
   "source": [
    "**Exercise 3.** First install spaCy: https://spacy.io/usage  using the link we provided, according to your operating system. Perform entity recognition using spaCy https://spacy.io/usage/linguistic-features#named-entities on the file example-nobel.txt . Compute the F1 score of the techniques from Exercise 1, 2 and of spaCy on the task of finding entities in the file example-nobel.txt. Note that for this you need to create the gold standard, i.e. the correct annotations. Do not take into account the entities of type DATE returned by spaCy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95b11ceb-23f9-4304-be7f-03f6565d58fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution exercise 3\n",
    "import spacy\n",
    "\n",
    "file_name = \"./data/example-nobel.txt\"\n",
    "\n",
    "def spacy_track(file_name):\n",
    "    file = open(file_name, 'r')\n",
    "    for line in file:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(line)\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ != \"DATE\":\n",
    "                print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "#spacy_track(file_name)\n",
    "\n",
    "def f_1():\n",
    "    file_tracked = \"./data/example-nobel.txt\"\n",
    "    file = open(file_tracked, 'r')\n",
    "    for line in file:\n",
    "        #Method 1\n",
    "        words_1 = track_with_trie(file_tracked) \n",
    "        \n",
    "        #Method 2\n",
    "        words_2 = re.findall(\"[A-Z][a-z]+ [A-Z][a-z]+\", line.strip())\n",
    "        \n",
    "        #Method 3\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(line)\n",
    "        words_3 = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ != \"DATE\":\n",
    "                words_3.append(ent.text)\n",
    "\n",
    "        #Ensuite pour le recall on regarde a quel point les words_i matchent avec words_abs avec la liste absolue des noms propres dans le fichier\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26687382-4993-41fd-baf5-6f54fb5f2fca",
   "metadata": {},
   "source": [
    "**Exercise 4.** spaCy does not have a relation extraction module! What can we do? Let's implement our own using the ReVerb algorithm that we saw during the class. In order to find the parts of speech needed to the algorithm, check this https://spacy.io/usage/linguistic-features#pos-tagging . Note that spaCy allows you to add regular expressions easily, such that you can implement ReVerb https://spacy.io/usage/rule-based-matching . Make sure to first split the text in sentences such that you can correctly identify the subject and object, by adapting the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1020f858-737d-4e61-a45f-aa298d1f26c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is the capital of France.\n",
      "Paris is located in France.\n",
      "Noun phrases: [('Paris', 0, 1), ('the capital', 2, 4), ('France', 5, 6), ('Paris', 7, 8), ('France', 11, 12)]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "doc = nlp(\"Paris is the capital of France. Paris is located in France.\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "    #print(type(sentence))\n",
    "print(\"Noun phrases:\", [(chunk.text, chunk.start, chunk.end) for chunk in\n",
    "                                doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8576abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)'\n",
    "            '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e963ce-7abc-4ceb-aa57-f8bceb6c33ea",
   "metadata": {},
   "source": [
    "Here is an example of the first part of the regular expression you need to write to get the predicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c07dd38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFunny : this works\\nresult = \"{} {}\".format(string1, string2)  # Format placeholders\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Funny : this works\n",
    "result = \"{} {}\".format(string1, string2)  # Format placeholders\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f154356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'\\ntext = read_file(\"./arthur.txt\")\\nprint(text)\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_file(file_path):\n",
    "    txt = \"\"\n",
    "    file = open(file_path, 'r')\n",
    "    for line in file:\n",
    "        txt += line.strip()\n",
    "    return txt\n",
    "\n",
    "''''\n",
    "text = read_file(\"./arthur.txt\")\n",
    "print(text)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73524f0d-932e-4cbf-a528-b806f3167b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris PROPN\n",
      "is AUX\n",
      "the DET\n",
      "capital NOUN\n",
      "of ADP\n",
      "France PROPN\n",
      ". PUNCT\n",
      "V 1 2 is\n",
      "VW*P 1 5 is the capital of\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#V\n",
    "pattern1 = [{\"POS\": {\"IN\":[\"AUX\",\"VERB\"]}}, {\"POS\": \"VERB\", \"OP\":\"?\"}, \n",
    "            {\"POS\": \"PART\", \"OP\":\"?\"}, {\"POS\": \"ADV\", \"OP\":\"?\"}]\n",
    "matcher.add(\"V\", [pattern1], greedy='LONGEST')\n",
    "\n",
    "#VW*P\n",
    "pattern2 = [{\"POS\": {\"IN\":[\"AUX\",\"VERB\"]}}, {\"POS\": \"VERB\", \"OP\":\"?\"}, \n",
    "            {\"POS\": \"PART\", \"OP\":\"?\"}, {\"POS\": \"ADV\", \"OP\":\"?\"},\n",
    "            \n",
    "            {\"POS\": {\"IN\":[\"NOUN\",\"ADJ\",\"ADV\",\"PRON\",\"DET\"]}, \"OP\":\"*\"},\n",
    "\n",
    "            {\"POS\": {\"IN\":[\"ADP\",\"PART\"]}}]\n",
    "matcher.add(\"VW*P\", [pattern2], greedy='LONGEST')\n",
    "\n",
    "#VP\n",
    "pattern3 = [{\"POS\": {\"IN\":[\"AUX\",\"VERB\"]}}, {\"POS\": \"VERB\", \"OP\":\"?\"}, \n",
    "            {\"POS\": \"PART\", \"OP\":\"?\"}, {\"POS\": \"ADV\", \"OP\":\"?\"},\n",
    "            {\"POS\": {\"IN\":[\"ADP\",\"PART\"]}}]\n",
    "matcher.add(\"VP\", [pattern3], greedy='LONGEST')\n",
    "\n",
    "text = read_file(\"./data/simplewiki.txt\")\n",
    "text = \"Paris is the capital of France.\"\n",
    "#print(text)\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2af20a-0c67-48ee-9368-5a78d8e1d217",
   "metadata": {},
   "source": [
    "Finish the code by adding the full regular expression from the class, V|VP|VW*P.\n",
    "First try your code on a small paragraph, e.g. on the sentences above you get the triples (Paris, is the capital of, France) and (Paris, is located in , France). Then test your algorithm on a sample from Wikipedia, contained in the file *simplewiki.txt*.\n",
    "\n",
    "Note that you can use part-of-speech tags to improve your NER algorithm that uses regular expression on capital letter (Exercise 2). Filter out words that are not nouns from your matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3694e9d-332e-4db7-9307-14485bbcf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed25ef2-5d56-4a9a-8452-3de8ee5f5011",
   "metadata": {},
   "source": [
    "**Exercise 5.** Filter the triples you extracted for the sample of Wikipedia such that you keep only the ones for which the subject is a named entity. \n",
    "You might have several mentions of the same entity, perform deduplication on these mentions (i.e. determine which mentions refer to the same object). For this, write a simple algorithm that given two entities in input, computes if they are the same using their names, and the label of the entities that is given when you run the named entity function of spaCy. If you find that mentions of entities with the same name that refer to different objects, modify the mention's name such that the mentions are distinct (eg: Europe_continent, Europe_person).  \n",
    "Create a graph in NetworkX using the triples your obtain and find the important nodes in this graph, using different measures of importance that we have seen during the first course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eca554-e597-495e-af27-38ea5a3b1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution exercise 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
