{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf477e9-842c-41c8-901b-728340e0b509",
   "metadata": {},
   "source": [
    "# Lab 4: Information Extraction\n",
    "\n",
    "\n",
    "**Exercise 1.** Implement a trie that allows the operations *insert* and *find*. Add all the entities from the file *winners.txt* to the trie. Using this trie, perform entity recognition in the text file *example-nobel.txt*. The files are on Moodle.\n",
    "\n",
    "You can use as starting point the following code: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106d618",
   "metadata": {},
   "source": [
    "Lien de la ressource pour les Regex : https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4bc536a-1bbc-4994-9a14-64a9931b9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrieNode:\n",
    "\t\"\"\"A node in the trie structure\"\"\"\n",
    "\n",
    "\tdef __init__(self, char):\n",
    "\t\t# the character stored in this node\n",
    "\t\tself.char = char\n",
    "\n",
    "\t\t# whether this can be the end of a word\n",
    "\t\tself.is_end = False\n",
    "\n",
    "\t\t# a dictionary of child nodes\n",
    "\t\t# keys are characters, values are nodes\n",
    "\t\tself.children = {}\n",
    "\n",
    "\n",
    "class Trie(object):\n",
    "\t\"\"\"The trie object\"\"\"\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tThe trie has at least the root node.\n",
    "\t\tThe root node does not store any character\n",
    "\t\t\"\"\"\n",
    "\t\tself.root = TrieNode(\"\")\n",
    "\n",
    "\tdef insert(self, word: str):\n",
    "\t\t\"\"\"Insert a word into the trie\"\"\"\n",
    "\n",
    "\t\t# TO COMPLETE\n",
    "\t\tnode = self.root\n",
    "\n",
    "\t\t#On avance au fur et a mesure\n",
    "\t\tfor i in range(len(word)):\n",
    "\t\t\t\n",
    "\t\t\tif word[i] in node.children:\n",
    "\t\t\t\tnode = node.children[word[i]]\n",
    "\t\t\telse:\n",
    "\t\t\t\tchild = TrieNode(word[i])\n",
    "\t\t\t\tnode.children[word[i]] = child\n",
    "\t\t\t\tnode = child\n",
    "\n",
    "\t\tnode.is_end = True\t#Le dernier noeud termine forcement\n",
    "\n",
    "\t\t\t\n",
    "\tdef find(self, x):\n",
    "\t\t\"\"\"Return a tuple of two boolean values:\n",
    "\t\t  - first value: true or false if the prefix is present or not in the trie.\n",
    "\t\t  - second value: true if the prefix is a word in the trie\n",
    "\t\t  Example:\n",
    "\t\t\t  Suppose Adams is in our trie t.\n",
    "\t\t\t  t.find(\"Adams\") returns (True, True)\n",
    "\t\t\t  t.find(\"Adam\") returns (True, False)\n",
    "\t\t  \"\"\"\n",
    "\t\t\n",
    "\t\t# TO COMPLETE\n",
    "\t\tword = x\n",
    "\t\tval_1, val_2 = False, False\n",
    "\t\tnode = self.root\n",
    "\t\tfor i in len(word):\n",
    "\t\t\tif word[i] in node.children:\n",
    "\t\t\t\tnode = node.children[word[i]]\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn False, False\n",
    "\t\t\n",
    "\t\treturn True, node.is_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa500e-ca57-46ae-8005-d764b583506d",
   "metadata": {},
   "source": [
    "**Exercise 2.** Supposing that all entities are written such that they start with a capital letter, perform entity recognition in the text file *example-nobel.txt* using regular expression. You should also be able to capture with the regex that an entity can have 2 or more names, such as Barack Obama. For this, first follow this tutorial https://www.w3schools.com/python/python_regex.asp on the **re library** in Python. When you have an idea for a regex, you can test it using this website https://regex101.com/ . Note the website even generates the corresponding Python code for you! You might need to read a bit about repeating a group https://www.regular-expressions.info/captureall.html ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51cd66-a328-4457-aef0-ca6e546da82f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 4113: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     30\u001b[39m             trie.insert(word)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trie\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m trie = \u001b[43mbuild_trie_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data/winners.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m#On cherche maintenant les noms propres du fichier example-nobel.txt\u001b[39;00m\n\u001b[32m     36\u001b[39m file = \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m./data/example-nobel.txt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mbuild_trie_from_file\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     24\u001b[39m trie = Trie()\n\u001b[32m     25\u001b[39m file = Path(file_path)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.splitlines():\n\u001b[32m     27\u001b[39m     clean_line = line.strip()\n\u001b[32m     28\u001b[39m     words = clean_line.split()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\pathlib.py:1028\u001b[39m, in \u001b[36mPath.read_text\u001b[39m\u001b[34m(self, encoding, errors)\u001b[39m\n\u001b[32m   1026\u001b[39m encoding = io.text_encoding(encoding)\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.open(mode=\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=encoding, errors=errors) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m-> \u001b[39m\u001b[32m1028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\encodings\\cp1252.py:23\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'charmap' codec can't decode byte 0x8d in position 4113: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Solution exercise 2\n",
    "def build_trie_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file and returns a trie containing all the words in the file.\n",
    "    Each word is assumed to be on a separate line.\n",
    "    \"\"\"\n",
    "    trie = Trie()\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            clean_line = line.strip()\n",
    "            words = clean_line.split()\n",
    "            if word in words:\n",
    "                trie.insert(word)\n",
    "    return trie\n",
    "\n",
    "trie = build_trie_from_file(\"./data/winners.txt\")\n",
    "#On cherche maintenant les noms propres du fichier example-nobel.txt\n",
    "file = open(\"./data/example-nobel.txt\", 'r')\n",
    "for line in file:\n",
    "    words = re.findall(r'\\\\b[A-Z][a-z]*\\\\b', line)\n",
    "    for word in words:\n",
    "        print(word, trie.find(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547141be-2d83-440a-9c93-e42d6d573a3a",
   "metadata": {},
   "source": [
    "**Exercise 3.** First install spaCy: https://spacy.io/usage  using the link we provided, according to your operating system. Perform entity recognition using spaCy https://spacy.io/usage/linguistic-features#named-entities on the file example-nobel.txt . Compute the F1 score of the techniques from Exercise 1, 2 and of spaCy on the task of finding entities in the file example-nobel.txt. Note that for this you need to create the gold standard, i.e. the correct annotations. Do not take into account the entities of type DATE returned by spaCy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b11ceb-23f9-4304-be7f-03f6565d58fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26687382-4993-41fd-baf5-6f54fb5f2fca",
   "metadata": {},
   "source": [
    "**Exercise 4.** spaCy does not have a relation extraction module! What can we do? Let's implement our own using the ReVerb algorithm that we saw during the class. In order to find the parts of speech needed to the algorithm, check this https://spacy.io/usage/linguistic-features#pos-tagging . Note that spaCy allows you to add regular expressions easily, such that you can implement ReVerb https://spacy.io/usage/rule-based-matching . Make sure to first split the text in sentences such that you can correctly identify the subject and object, by adapting the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1020f858-737d-4e61-a45f-aa298d1f26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "doc = nlp(\"Paris is the capital of France. Paris is located in France.\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)\n",
    "    print(\"Noun phrases:\", [(chunk.text, chunk.start, chunk.end) for chunk in\n",
    "                                doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e963ce-7abc-4ceb-aa57-f8bceb6c33ea",
   "metadata": {},
   "source": [
    "Here is an example of the first part of the regular expression you need to write to get the predicate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73524f0d-932e-4cbf-a528-b806f3167b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"POS\": {\"IN\":[\"AUX\",\"VERB\"]}}, {\"POS\": \"VERB\", \"OP\":\"?\"}, \n",
    "            {\"POS\": \"PART\", \"OP\":\"?\"}, {\"POS\": \"ADV\", \"OP\":\"?\"}]\n",
    "matcher.add(\"V\", [pattern1], greedy='LONGEST')\n",
    "doc = nlp(\"Paris is the capital of France.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2af20a-0c67-48ee-9368-5a78d8e1d217",
   "metadata": {},
   "source": [
    "Finish the code by adding the full regular expression from the class, V|VP|VW*P.\n",
    "First try your code on a small paragraph, e.g. on the sentences above you get the triples (Paris, is the capital of, France) and (Paris, is located in , France). Then test your algorithm on a sample from Wikipedia, contained in the file *simplewiki.txt*.\n",
    "\n",
    "Note that you can use part-of-speech tags to improve your NER algorithm that uses regular expression on capital letter (Exercise 2). Filter out words that are not nouns from your matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3694e9d-332e-4db7-9307-14485bbcf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed25ef2-5d56-4a9a-8452-3de8ee5f5011",
   "metadata": {},
   "source": [
    "**Exercise 5.** Filter the triples you extracted for the sample of Wikipedia such that you keep only the ones for which the subject is a named entity. \n",
    "You might have several mentions of the same entity, perform deduplication on these mentions (i.e. determine which mentions refer to the same object). For this, write a simple algorithm that given two entities in input, computes if they are the same using their names, and the label of the entities that is given when you run the named entity function of spaCy. If you find that mentions of entities with the same name that refer to different objects, modify the mention's name such that the mentions are distinct (eg: Europe_continent, Europe_person).  \n",
    "Create a graph in NetworkX using the triples your obtain and find the important nodes in this graph, using different measures of importance that we have seen during the first course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eca554-e597-495e-af27-38ea5a3b1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution exercise 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
